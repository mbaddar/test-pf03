---
pagetitle: "W2 D5"
title: "Data Work (ETL + EDA)"
subtitle: "AI Professionals Bootcamp | Week 2"
date: 2025-12-25
---

## Policy: GenAI usage

- ✅ Allowed: **clarifying questions** (definitions, error explanations)
- ❌ Not allowed: generating code, writing solutions, or debugging by copy-paste
- If unsure: ask the instructor first

::: callout-tip
**In this course:** you build skill by typing, running, breaking, and fixing.
:::

# Day 5: Ship the Pipeline + Reporting Handoff

**Goal:** package your Week 2 work as a **job-ready handoff**: reproducible ETL → processed datasets → EDA notebook → exported figures → a short written summary with caveats.

::: {.muted}
Bootcamp • SDAIA Academy
:::

::: {.notes}
Say: “Today is about being employable: someone else can run your repo and trust your outputs.”
Do: show end-state: run_etl -> processed files -> notebook -> figures -> summary.md.
Ask: “If I clone your repo, can I reproduce your numbers in 5 minutes?”
:::

---

## Today’s Flow

* **Session 1 (60m):** ETL pipeline patterns (run_etl, config, logging, QA)
* *Asr Prayer (20m)*
* **Session 2 (60m):** Outputs + metadata + handoff quality (what to ship)
* *Maghrib Prayer (20m)*
* **Session 3 (60m):** Optional DuckDB “SQL view layer” + final checks
* *Isha Prayer (20m)*
* **Hands-on (120m):** Build `etl.py` + run metadata + `reports/summary.md` + final repo cleanup

---

## Learning Objectives

By the end of today, you can: 

* turn your work into a reproducible **ETL pipeline** (`run_etl()`)
* add lightweight logging and **fail-fast QA checks**
* write **idempotent** processed outputs (safe to rerun)
* generate a minimal **run metadata** JSON (inputs + counts)
* produce a short `reports/summary.md` with findings + caveats + next steps
* ensure a new person can clone and run your repo successfully

---

## Warm-up (5 minutes)

Confirm Day 3 + Day 4 artifacts exist.

**macOS/Linux**

```bash
source .venv/bin/activate
python scripts/run_day3_build_analytics.py
ls -la data/processed | head
```

**Windows PowerShell**

```powershell
.\\.venv\\Scripts\\Activate.ps1
python scripts\\run_day3_build_analytics.py
dir data\\processed
```

**Checkpoint:** `analytics_table.parquet` exists and Day 3 script runs.

---

## Definition of “done” for Week 2 (submit-ready)

You will ship: 

* **Reproducible ETL** that reads from `data/raw/` (and/or `data/cache/`)
* **Validations**: required columns, non-empty, unique keys, join validation, basic ranges
* **Idempotent outputs** to `data/processed/` (prefer Parquet)
* **EDA notebook** reading only from `data/processed/`
* **Exported figures** in `reports/figures/`
* **Written summary** in `reports/summary.md` (findings + caveats + next steps)

# Session 1

::: {.muted}
ETL pipeline patterns (run_etl, config, logging, QA)
:::

---

## Session 1 objectives

By the end of this session, you can: 

* define Extract / Transform / Load in this bootcamp context
* structure an ETL module with:

  * `load_inputs()`
  * `transform()`
  * `load_outputs()`
  * `run_etl()`
* add logging for row counts and paths
* make outputs idempotent (overwrite, don’t append)

---

## Context: notebooks are not pipelines

Notebooks are great for exploration.

Pipelines are needed when:

* you re-run many times
* you share work with teammates
* you want reproducibility and trust

Today: convert “daily scripts” into a clean ETL module. 

---

## Concept: ETL definition (Week 2)

ETL in our offline-first workflow: 

* **Extract:** read from `data/raw` and/or `data/cache`
* **Transform:** pure transforms (`df -> df`), deterministic
* **Load:** write to `data/processed` + minimal metadata

---

## A good ETL has these properties

* idempotent outputs (safe reruns)
* logs what it did (inputs/outputs/counts)
* checks assumptions (fail fast)
* keeps I/O separate from transforms
* produces stable, analysis-ready tables 

---

## Concept: minimal ETL module structure

In `src/bootcamp_data/etl.py`:

* `ETLConfig` dataclass
* `load_inputs(cfg)`
* `transform(orders, users, cfg)` *(or similar)*
* `load_outputs(df, cfg)`
* `write_run_meta(cfg, ...)`
* `run_etl(cfg)`

---

## Example: ETL skeleton (shape only)

```python
def load_inputs(cfg): ...
def transform(orders, users): ...
def load_outputs(df, cfg): ...
def run_etl(cfg): ...
```

::: {.muted}
We keep it small and shippable (no orchestration frameworks).
:::

---

## Concept: logging (why it matters)

Logging gives you:

* visibility (row counts)
* debugging info (paths, step names)
* audit trail (what ran, when)

Avoid “print-only pipelines” for anything beyond demos. 

---

## Example: minimal logging

```python
import logging
log = logging.getLogger(__name__)

logging.basicConfig(level=logging.INFO, format="%(levelname)s %(name)s: %(message)s")
log.info("Loading inputs...")
```

---

## Quick Check

**Question:** What 2 things should every ETL run log?

. . .

**Answer:** row counts and output paths (at minimum). 

---

## Concept: where QA checks go

Run checks:

* right after loading inputs
* right after major transforms (especially joins)
* before writing outputs

This catches problems closest to their cause. 

---

## Example: a QA checkpoint placement

```python
orders, users = load_inputs(cfg)
require_columns(orders, [...])
assert_unique_key(users, "user_id")

out = transform(orders, users)
assert len(out) == len(orders)  # join sanity

load_outputs(out, cfg)
```

---

## Micro-exercise: name 3 checks you will include (3 minutes)

Write 3 checks you will include in your `transform()`.

**Checkpoint:** each check has a clear purpose.

---

## Solution: examples of high ROI checks

* `require_columns(orders, ["order_id","user_id","amount","created_at","status"])`
* `assert_unique_key(users, "user_id")`
* `assert len(joined) == len(orders)` after left join

(Plus range checks for amount/quantity if needed.) 

---

## Session 1 recap

* ETL = Extract → Transform → Load (offline-first)
* Keep transforms pure and deterministic
* Log counts + paths
* Fail fast with lightweight QA checks
* Idempotent outputs are mandatory for reruns 

# Asr break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}

## 20 minutes

**When you return:** we’ll define what “handoff quality” looks like and what to ship.

# Session 2

::: {.muted}
Outputs + metadata + handoff quality (what to ship)
:::

---

## Session 2 objectives

By the end of this session, you can: 

* define “handoff quality” for a data project
* produce minimal run metadata (config + counts)
* export a schema/missingness summary (optional)
* write a clear `reports/summary.md` with caveats

---

## Context: your future teammate is the user

A good repo answers:

* “How do I run this?”
* “Where are outputs?”
* “What do these columns mean?”
* “What can go wrong?”

This is what hiring managers look for too.

---

## Concept: handoff quality checklist

Your Week 2 handoff should include: 

* processed datasets (Parquet)
* EDA notebook (reads processed)
* exported figures
* written summary + caveats
* minimal metadata so numbers can be reproduced

---

## Concept: minimal run metadata

Run metadata can be a small JSON file with:

* config (input paths, output paths)
* row counts
* key quality stats (missing timestamps, match rate, etc.)

This makes reruns auditable. 

---

## Example: run metadata JSON

```json
{
  "rows_out": 12345,
  "inputs": {
    "orders_raw": "data/raw/orders.csv",
    "users_raw": "data/raw/users.csv"
  },
  "outputs": {
    "analytics_table": "data/processed/analytics_table.parquet"
  }
}
```

---

## Example: `write_run_meta(cfg, rows_out=...)` pattern 

```python
import json
from dataclasses import asdict

def write_run_meta(cfg, *, rows_out: int, path):
    path.parent.mkdir(parents=True, exist_ok=True)
    meta = {"config": {k: str(v) for k, v in asdict(cfg).items()}, "rows_out": rows_out}
    path.write_text(json.dumps(meta, indent=2), encoding="utf-8")
```

---

## Micro-exercise: what else would you record? (3 minutes)

Add **two more fields** you would record in run metadata.

**Checkpoint:** two fields that would help debugging or reproducibility.

---

## Solution: helpful metadata fields

Examples:

* `missing_created_at_after_parse`
* `country_match_rate_after_join`
* `git_commit` (if you want, optional)
* `run_timestamp_utc`

Keep it minimal and useful. 

---

## Concept: written summary structure

Your `reports/summary.md` should include: 

* key findings (bulleted + quantified)
* definitions (metrics + filters)
* caveats (missingness, duplicates, join coverage, outliers)
* recommended next steps / questions

---

## Example: summary template (what to write)

```markdown
# Week 2 Summary

## Key findings
- ...
- ...

## Definitions
- Revenue = ...
- Refund rate = ...

## Data quality caveats
- Missingness: ...
- Joins: ...
- Outliers: ...

## Next questions
- ...
```

---

## Micro-exercise: write 3 caveats (5 minutes)

Write 3 caveats you must mention.

**Checkpoint:** each caveat is concrete and tied to your pipeline.

---

## Solution: common caveats (examples)

* Missingness in `amount` due to invalid parsing → totals may be understated.
* Some orders may not match users (join coverage < 100%) → segment results may be biased.
* Outliers exist in `amount`; charts used winsorized values for readability. 

---

## Session 2 recap

* Handoff quality = reproducible + understandable + auditable
* Add run metadata (config + counts + a few key stats)
* Ship a short summary with findings + caveats + next steps 

# Maghrib break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}

## 20 minutes

**When you return:** optional DuckDB SQL + final checks before shipping.

# Session 3

::: {.muted}
Optional DuckDB “SQL view layer” + final checks
:::

---

## Session 3 objectives

By the end of this session, you can: 

* explain when SQL (DuckDB) is useful locally
* run a simple SQL aggregation on Parquet
* compare SQL output vs pandas output (sanity check)
* perform final repo checks before submission

---

## Context: sometimes SQL is the fastest expression

DuckDB lets you query local Parquet/CSV:

* no server
* offline
* quick analytics “view layer”

We treat this as optional — pandas remains the core. 

---

## Concept: DuckDB as a view layer

Use DuckDB when:

* the query is simpler in SQL than pandas
* you want a second way to verify results
* you want to practice SQL fundamentals without heavy setup 

---

## Example: query Parquet with DuckDB (minimal) 

```python
import duckdb
from pathlib import Path

def query_parquet(path: Path, sql: str):
    con = duckdb.connect()
    con.execute("CREATE VIEW t AS SELECT * FROM read_parquet(?)", [str(path)])
    return con.execute(sql).df()
```

---

## Micro-exercise: write a SQL query (4 minutes)

Write a SQL query for:

* revenue by country (sum of amount)
* sorted by revenue desc

**Checkpoint:** you wrote valid SQL text.

---

## Solution: SQL query example

```sql
SELECT
  country,
  COUNT(*) AS n,
  SUM(amount) AS revenue
FROM t
GROUP BY 1
ORDER BY revenue DESC
```

---

## Quick Check

**Question:** What is DuckDB reading from in this setup?

. . .

**Answer:** directly from the local Parquet file (no database server). 

---

## Final checks before shipping

Before you submit, confirm: 

* Day 5 ETL runs from a clean terminal
* outputs land in `data/processed/`
* notebook reads only processed data
* figures exported
* summary written
* README includes “How to run”

---

## Session 3 recap

* DuckDB is optional but useful for fast local SQL analytics
* It can help validate pandas results
* Final checklists catch “it works on my laptop” issues 

# Isha break {background-image='{{< brand logo anim >}}' background-opacity='0.1'}

## 20 minutes

**When you return:** we’ll implement `etl.py`, generate metadata, and write `reports/summary.md`.

# Hands-on

::: {.muted}
Build: `etl.py` + run metadata + summary + repo cleanup
:::

---

## Hands-on success criteria (today)

By the end, you should have: 

* `src/bootcamp_data/etl.py` with `run_etl(cfg)` producing processed outputs
* `scripts/run_etl.py` (thin entrypoint)
* `data/processed/_run_meta.json` created each run
* `reports/summary.md` completed
* `README.md` updated with “How to run ETL” and “Where outputs go”
* final commit pushed to GitHub

---

## Target deliverables (what must exist)

```text
src/bootcamp_data/
  etl.py
scripts/
  run_etl.py
data/processed/
  orders_clean.parquet
  users.parquet
  analytics_table.parquet
  _run_meta.json
notebooks/
  eda.ipynb
reports/
  figures/...
  summary.md
README.md
```

---

## If `import bootcamp_data` fails (fix in 2 minutes) {.smaller}

**Symptom:** `ModuleNotFoundError: No module named 'bootcamp_data'`

Pick **one** of these fixes:

1) **Recommended:** install your project in editable mode (so `src/` is importable)

```bash
pip install -e .
```

2) **Quick run without install:** set `PYTHONPATH=src` when running scripts

**macOS/Linux**
```bash
PYTHONPATH=src python scripts/run_etl.py
```

**Windows PowerShell**
```powershell
$env:PYTHONPATH="src"
python scripts\run_etl.py
```

3) **Script bootstrap (also OK):** add `ROOT/'src'` to `sys.path` before importing (see the Day 5 script example).

---

## Task 1 — Create `ETLConfig` (15 minutes)

Create `src/bootcamp_data/etl.py` with:

* `ETLConfig` dataclass containing key input/output paths:

  * raw orders/users
  * processed outputs (orders_clean, users, analytics_table)
  * run metadata path

**Checkpoint:** you can import `ETLConfig`.

---

## Solution — `ETLConfig` (example) {.smaller}

```python
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class ETLConfig:
    root: Path
    raw_orders: Path
    raw_users: Path
    out_orders_clean: Path
    out_users: Path
    out_analytics: Path
    run_meta: Path
```

---

## Task 2 — Implement `load_inputs()` (10 minutes)

In `etl.py` implement:

* read raw CSVs using your `io.py` helpers

**Checkpoint:** `load_inputs(cfg)` returns `(orders_raw, users_raw)` DataFrames.

---

## Solution — `load_inputs` (example)

```python
import pandas as pd
from bootcamp_data.io import read_orders_csv, read_users_csv

def load_inputs(cfg: ETLConfig) -> tuple[pd.DataFrame, pd.DataFrame]:
    orders = read_orders_csv(cfg.raw_orders)
    users = read_users_csv(cfg.raw_users)
    return orders, users
```

---

## Task 3 — Implement `transform()` (35 minutes)

In `etl.py`, implement transform steps (reuse your helpers): 

* checks: required columns, non-empty
* schema enforcement
* status normalization + mapping
* missing flags
* parse datetime + time parts

---

## Task 3 — Implement `transform()` (35 minutes)

* assert uniqueness on users
* safe left join orders → users (`validate="many_to_one"`)
* winsorize amount + outlier flag
* post-join row count sanity

**Checkpoint:** `transform(orders, users)` returns the final analytics table.

---

## Concept: `transform()` = Day 2 + Day 3 work composed

You're not inventing new logic — you're **wiring yesterday’s helpers together**:

**Day 2 (cleaning)**

* `enforce_schema(df)` (dtypes)
* `normalize_text(...)` + `apply_mapping(..., mapping)` (categoricals)
* `add_missing_flags(df, cols=[...])` (keep missingness visible)

---

## Concept: `transform()` = Day 2 + Day 3 work composed

**Day 3 (time + joins + outliers)**

* `parse_datetime(df, col="created_at", utc=True)`
* `add_time_parts(df, ts_col="created_at")`
* `safe_left_join(..., validate="many_to_one")`
* `winsorize(df["amount"])` + `add_outlier_flag(df, "amount")`

**End-state:** one row per order in `analytics_table.parquet`, enriched with user fields + time parts + outlier/winsor columns.

---

## Hint — keep transforms deterministic + testable

* Use pure functions
* Use `.pipe()` for readability
* Keep I/O out of transform
* Print/log row counts and key stats

---

## Solution — `transform()` (example) {.smaller}

```python
import pandas as pd

from bootcamp_data.quality import require_columns, assert_non_empty, assert_unique_key
from bootcamp_data.transforms import (
    enforce_schema,
    add_missing_flags,
    normalize_text,
    apply_mapping,
    parse_datetime,
    add_time_parts,
    winsorize,
    add_outlier_flag,
)
from bootcamp_data.joins import safe_left_join


def transform(orders_raw: pd.DataFrame, users: pd.DataFrame) -> pd.DataFrame:
    # Fail-fast checks (before doing work)
    require_columns(
        orders_raw,
        ["order_id", "user_id", "amount", "quantity", "created_at", "status"],
    )
    #Code continues on next slide
```

---

## Solution — `transform()` (example) {.smaller}

```python
    require_columns(users, ["user_id", "country", "signup_date"])
    assert_non_empty(orders_raw, "orders_raw")
    assert_non_empty(users, "users")

    # The “one” side must be unique for a many→one join
    assert_unique_key(users, "user_id")

    status_map = {"paid": "paid", "refund": "refund", "refunded": "refund"}

    orders = (
        orders_raw.pipe(enforce_schema)
        .assign(
            status_clean=lambda d: apply_mapping(normalize_text(d["status"]), status_map)
        )
        .pipe(add_missing_flags, cols=["amount", "quantity"])
        .pipe(parse_datetime, col="created_at", utc=True)
        .pipe(add_time_parts, ts_col="created_at")
    )
```

## Solution — `transform()` (example) {.smaller}

```python
    joined = safe_left_join(
        orders,
        users,
        on="user_id",
        validate="many_to_one",
        suffixes=("", "_user"),
    )

    # Left join should not change row count
    assert len(joined) == len(orders), "Row count changed (join explosion?)"

    # Outliers: keep raw `amount`, add winsorized + outlier flag for analysis
    joined = joined.assign(amount_winsor=winsorize(joined["amount"]))
    joined = add_outlier_flag(joined, "amount", k=1.5)

    return joined
```

---

## Task 4 — Implement `load_outputs()` + run metadata (25 minutes)

In `etl.py`:

* write outputs to `data/processed/` (Parquet)
* write `_run_meta.json` including:

  * row counts
  * missing timestamps
  * join match rate for `country`
  * config paths

**Checkpoint:** running ETL creates `_run_meta.json`.

---

## Solution — write outputs + metadata (example) {.smaller}

```python
import json
import logging
from dataclasses import asdict

import pandas as pd

from bootcamp_data.io import write_parquet

log = logging.getLogger(__name__)


def load_outputs(*, analytics: pd.DataFrame, users: pd.DataFrame, cfg: ETLConfig) -> None:
    """Write processed artifacts (idempotent)."""
    write_parquet(users, cfg.out_users)
    write_parquet(analytics, cfg.out_analytics)

    # Optional (but nice): write an orders-only table by dropping user-side columns
    user_side_cols = [c for c in users.columns if c != "user_id"]
    cols_to_drop = [c for c in user_side_cols if c in analytics.columns] + [
        c for c in analytics.columns if c.endswith("_user")
    ]
    orders_clean = analytics.drop(columns=cols_to_drop, errors="ignore")
    write_parquet(orders_clean, cfg.out_orders_clean)
```

---

## Solution — write outputs + metadata (example) {.smaller}

```python
def write_run_meta(
    cfg: ETLConfig, *, orders_raw: pd.DataFrame, users: pd.DataFrame, analytics: pd.DataFrame
) -> None:
    missing_created_at = int(analytics["created_at"].isna().sum()) if "created_at" in analytics.columns else None
    country_match_rate = (
        1.0 - float(analytics["country"].isna().mean())
        if "country" in analytics.columns
        else None
    )

    meta = {
        "rows_in_orders_raw": int(len(orders_raw)),
        "rows_in_users": int(len(users)),
        "rows_out_analytics": int(len(analytics)),
        "missing_created_at": missing_created_at,
        "country_match_rate": country_match_rate,
        "config": {k: str(v) for k, v in asdict(cfg).items()},
    }

    cfg.run_meta.parent.mkdir(parents=True, exist_ok=True)
    cfg.run_meta.write_text(json.dumps(meta, indent=2), encoding="utf-8")
```

---

## Solution — write outputs + metadata (example) {.smaller}

```python
def run_etl(cfg: ETLConfig) -> None:
    logging.basicConfig(level=logging.INFO, format="%(levelname)s %(name)s: %(message)s")

    log.info("Loading inputs")
    orders_raw, users = load_inputs(cfg)

    log.info("Transforming (orders=%s, users=%s)", len(orders_raw), len(users))
    analytics = transform(orders_raw, users)

    log.info("Writing outputs to %s", cfg.out_analytics.parent)
    load_outputs(analytics=analytics, users=users, cfg=cfg)

    log.info("Writing run metadata: %s", cfg.run_meta)
    write_run_meta(cfg, orders_raw=orders_raw, users=users, analytics=analytics)
```

---

## Task 5 — Create `scripts/run_etl.py` entrypoint (15 minutes)

Create a thin script that:

* defines `ROOT`
* creates `ETLConfig` with correct paths
* calls `run_etl(cfg)`

**Checkpoint:** `python scripts/run_etl.py` runs end-to-end.

---

## Solution — `scripts/run_etl.py` (example) {.smaller}

```python
from __future__ import annotations

import sys
from pathlib import Path

# Allows: `python scripts/run_etl.py` without installing the package
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))

from bootcamp_data.etl import ETLConfig, run_etl


def main() -> None:
    cfg = ETLConfig(
        root=ROOT,
        raw_orders=ROOT / "data" / "raw" / "orders.csv",
        raw_users=ROOT / "data" / "raw" / "users.csv",
        out_orders_clean=ROOT / "data" / "processed" / "orders_clean.parquet",
        out_users=ROOT / "data" / "processed" / "users.parquet",
        out_analytics=ROOT / "data" / "processed" / "analytics_table.parquet",
        run_meta=ROOT / "data" / "processed" / "_run_meta.json",
    )
    run_etl(cfg)


if __name__ == "__main__":
    main()
```

---

## Task 6 — Write `reports/summary.md` (25 minutes)

Write a short summary with: 

* key findings (bulleted + numbers)
* definitions (metrics + filters)
* data quality caveats
* next questions

**Checkpoint:** `reports/summary.md` is complete and readable.

---

## Solution — summary template (copy/paste)

```markdown
# Week 2 Summary — ETL + EDA

## Key findings
- Finding 1 (quantified): ...
- Finding 2 (quantified): ...
- Finding 3 (quantified): ...

## Definitions
- Revenue = sum(amount) over ...
- Refund rate = refunds / total orders, where refund = status_clean == "refund"
- Time window = ...

## Data quality caveats
- Missingness: ...
- Duplicates: ...
- Join coverage: ...
- Outliers: ...

## Next questions
- ...
- ...
```

---

## Task 7 — README “How to run” (15 minutes)

Update `README.md`:

* setup steps
* how to run ETL
* where outputs go
* how to run the EDA notebook

**Checkpoint:** a new person can follow README to reproduce outputs.

---

## Solution — README snippet (example)

```markdown
## Setup
python -m venv .venv
# activate venv
pip install -r requirements.txt
pip install -e .

## Run ETL
python scripts/run_etl.py

## Outputs
- data/processed/orders_clean.parquet
- data/processed/users.parquet
- data/processed/analytics_table.parquet
- data/processed/_run_meta.json
- reports/figures/*.png

## EDA
Open notebooks/eda.ipynb and run all cells.
```

---

## Git checkpoint (5 minutes)

Final commit + push:

Suggested message:

* `"w2d5: ship etl + run metadata + summary handoff"`

**Checkpoint:** GitHub has your final Week 2 state.

---

## Solution — git commands

```bash
git add -A
git commit -m "w2d5: ship etl + run metadata + summary handoff"
git push
```

---

## Final submission checklist

Before you submit, confirm: 

* [ ] `python scripts/run_etl.py` runs cleanly
* [ ] `data/processed/analytics_table.parquet` is produced
* [ ] `data/processed/_run_meta.json` exists
* [ ] `notebooks/eda.ipynb` reads only processed data
* [ ] `reports/figures/` has exports
* [ ] `reports/summary.md` has findings + caveats
* [ ] README explains how to reproduce

---

## Exit Ticket

In 1–2 sentences:

**If a teammate clones your repo, what are the first 2 commands they should run — and what should they see?** 

---

## What to do after class (Week 2 submission)

**Due:** end of today

1. Push final code + notebook + reports to GitHub
2. Submit your repo link
3. Include a screenshot showing:

   * `data/processed/_run_meta.json`
   * `reports/figures/` exports

**Deliverable:** GitHub repo link + screenshot collage (meta + figures + summary.md).

::: callout-tip
Your Week 2 project is now a portfolio artifact. Make it easy to run.
:::

# Thank You! {background-image='{{< brand logo anim >}}' background-opacity='0.1'}

<div style="width: 300px">{{< brand logo full >}}</div>
